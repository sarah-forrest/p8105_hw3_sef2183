p8105_hw3_sef2183
================
Sarah Forrest
2022-10-06

# Problem 0

``` r
library(tidyverse)
library(patchwork)
```

# Problem 1

``` r
library(p8105.datasets)
data("instacart")
```

## Description of the dataset:

-   Size and structure of the data
-   Describe key variables
-   Illustrative examples of observations.

## Dataset questions (commenting on the results of each):

-   How many aisles are there, and which aisles are the most items
    ordered from?
-   Make a plot that shows the number of items ordered in each aisle,
    limiting this to aisles with more than 10000 items ordered.
-   Arrange aisles sensibly, and organize your plot so others can read
    it.
-   Make a table showing the three most popular items in *each of the
    aisles* “baking ingredients”, “dog food care”, and “packaged
    vegetables fruits”. Include the number of times each item is ordered
    in your table.
-   Make a table showing the mean hour of the day at which Pink Lady
    Apples and Coffee Ice Cream are ordered on each day of the week;
    format this table for human readers (i.e. produce a 2 x 7 table).

# Problem 2

Below, the accelerometer data from `accel_data.csv` is read in and the
variable names are cleaned using `clean_names()`. Then, `pivot_longer()`
is used to manipulate the dataset from wide to long format. To do this,
all variable names beginning with “activity” (`activity_1` to
`activity_1440`) were transformed into 2 different variables: one
denoting the minute of the day and one denoting the activity measure.The
variable names were changed to `minute` and the prefix “activity\_” was
removed so that the variable values contained only the minute number,
and the values were changed to `activity_count`, indicating the amount
of activity read by the accelerometer during each 1 minute window.
Finally, a new variable called `weekday_weekend` was created to indicate
whether the day of the week is a weekday or a weekend day.

``` r
accel_df = 
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    starts_with("activity"),
    names_to = "minute",
    names_prefix = "activity_", 
    values_to = "activity_count") %>%
  mutate(weekday_weekend = case_when(day == "Monday" ~ "weekday",
                                     day == "Tuesday" ~ "weekday",
                                     day == "Wednesday" ~ "weekday",
                                     day == "Thursday" ~ "weekday",
                                     day == "Friday" ~ "weekday",
                                     day == "Saturday" ~ "weekend",
                                     day == "Sunday" ~ "weekend"))

# What is meant by encode data with reasonable variable classes
```

## Dataset description

-   The variables in the resulting dataset include: `week`, `day_id`
    (day count number), `day` (day of the week), `minute`,
    `activity_count`, and `weekday_weekend` (either weekday or weekend)
-   The resulting dataset contains 50400 rows/observations and 6
    columns/variables.

## Table of total activity over each day

Below, the tidied accelerometer dataset was aggregated across minutes
using `group_by()` to group the data together by the `DAY_ID` variable.
Then a `total_activity` variable was created by summing the
`activity_count` variables for each 1 minute window throughout the day
to indicate the total activity read by the accelerometer during each day
of the study. Finally, only the relevant variables `day_id`, `week`,
`day`, and `total_activity` were selected from the dataset using
`select()` and a table was created to show the daily totals for each of
the 35 days of the study using `kable()` from the `knitr` package.

``` r
accel_df %>%
  group_by(day_id, week, day) %>%
  summarize(
    total_activity = sum(activity_count)) %>% 
  select(day_id, week, day, total_activity) %>% 
  knitr::kable()
```

| day_id | week | day       | total_activity |
|-------:|-----:|:----------|---------------:|
|      1 |    1 | Friday    |      480542.62 |
|      2 |    1 | Monday    |       78828.07 |
|      3 |    1 | Saturday  |      376254.00 |
|      4 |    1 | Sunday    |      631105.00 |
|      5 |    1 | Thursday  |      355923.64 |
|      6 |    1 | Tuesday   |      307094.24 |
|      7 |    1 | Wednesday |      340115.01 |
|      8 |    2 | Friday    |      568839.00 |
|      9 |    2 | Monday    |      295431.00 |
|     10 |    2 | Saturday  |      607175.00 |
|     11 |    2 | Sunday    |      422018.00 |
|     12 |    2 | Thursday  |      474048.00 |
|     13 |    2 | Tuesday   |      423245.00 |
|     14 |    2 | Wednesday |      440962.00 |
|     15 |    3 | Friday    |      467420.00 |
|     16 |    3 | Monday    |      685910.00 |
|     17 |    3 | Saturday  |      382928.00 |
|     18 |    3 | Sunday    |      467052.00 |
|     19 |    3 | Thursday  |      371230.00 |
|     20 |    3 | Tuesday   |      381507.00 |
|     21 |    3 | Wednesday |      468869.00 |
|     22 |    4 | Friday    |      154049.00 |
|     23 |    4 | Monday    |      409450.00 |
|     24 |    4 | Saturday  |        1440.00 |
|     25 |    4 | Sunday    |      260617.00 |
|     26 |    4 | Thursday  |      340291.00 |
|     27 |    4 | Tuesday   |      319568.00 |
|     28 |    4 | Wednesday |      434460.00 |
|     29 |    5 | Friday    |      620860.00 |
|     30 |    5 | Monday    |      389080.00 |
|     31 |    5 | Saturday  |        1440.00 |
|     32 |    5 | Sunday    |      138421.00 |
|     33 |    5 | Thursday  |      549658.00 |
|     34 |    5 | Tuesday   |      367824.00 |
|     35 |    5 | Wednesday |      445366.00 |

-   It’s very difficult to see any trends that are apparent from this
    table. I immediately noticed that the last 2 Saturdays in the study
    (days 24 and 31) had a much lower total activity count than the
    other days in the study. However, the previous Saturdays in the
    study had relatively similar total activity counts to the rest of
    the days in the study, so this isn’t a trend that can be seen
    through the entire dataset. Visualization may be required to see any
    trends, if they are apparent in this dataset.

## Plot of the 24-hour activity time courses for each day

Below, a single panel-plot was created using `ggplot()` to show the
24-hour activity time courses for each day, using color to indicate the
day of the week. `group_by()` was used to group the data by day of the
week, then `ggplot()` was called to create a scatterplot with `minute`
on the x axis and `activity_count` (data read from the accelerometer in
1 minute intervals) on the y axis.

``` r
accel_df %>%
  group_by(day) %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) + geom_point() + 
  theme(legend.position = "bottom")
```

![](p8105_hw3_sef2183_files/figure-gfm/problem%202%20plot-1.png)<!-- -->

This plot shows the individual’s activity course throughout the entire
1440 minutes within a 24-hour period for each day of the week throughout
the 5 weeks if the study observation period. However, since there are so
many minutes in a 24-hour period, the plot is messy and the values on
the x axis are difficult to read. Even with this difficulty, it is still
possible to see some patterns and trends in the data:

-   For all days of the week, there are lower activity counts that can
    be observed mid morning, at around 9:30am-10:30am (estimated,
    because exact minutes can not be read on the x axis)
-   On Tuesdays, the individual doesn’t appear to spend any minutes with
    activity counts over 3750, indicating that the individual seems to
    have less/lighter activity on Tuesdays.
-   On Sundays, the individual has most of their high activity counts
    later in the day compared to the other days of the week at around
    5:00pm (estimated)
-   On Thursdays, the individual has most of their high activity counts
    around midday at around 1:00pm (estimated)
-   On Fridays, the individual has most of their high activity counts
    earlier in the day at around 5:00am (estimated), with some high
    activity in the early afternoon as well at around 3:00pm (estimated)
-   On Mondays and Wednesdays, the individual has most of their high
    activity counts earlier in the day at around 5:00am (estimated).
-   On Saturdays, the individual has most of their high activity counts
    earlier in the day at around 5:00am (estimated) and a some at late
    at night as well, at around 12:00am (estimated)

# Problem 3

``` r
library(p8105.datasets)
data("ny_noaa")
```

## Description of the dataset:

-   The size of the dataset is 2,595,176 observations (rows) and
    variables 7 variables (columns)
-   The structure of the dataset is in long format, with 1 row for each
    observation date for each weather station.
-   Variables in the dataset include: `id` (weather station ID), `date`
    (observation date), `prcp` (precipitation measured in tenths of a
    mm), `snow` (snowfall measured in mm), `snwd` (snow depth measured
    in mm), `tmax` (maximum temperature measured in tenths of degrees
    C), and `tmin` (minimum temperature measured in tenths of degrees
    C).
-   The dataset has quite a bit of missing data, which has the potential
    to create issues for future analysis:
    -   There are `sum(is.na(ny_noaa$prcp))` missing observations for
        `prcp`
    -   There are `sum(is.na(ny_noaa$snow))` missing observations for
        `snow`
    -   There are `sum(is.na(ny_noaa$snwd))` missing observations for
        `snwd`
    -   There are `sum(is.na(ny_noaa$tmax))` missing observations for
        `tmax`
    -   There are `sum(is.na(ny_noaa$tmin))` missing observations for
        `tmin`

``` r
sum(is.na(ny_noaa$prcp)) 
## [1] 145838
sum(is.na(ny_noaa$snow))
## [1] 381221
sum(is.na(ny_noaa$snwd))
## [1] 591786
sum(is.na(ny_noaa$tmax))
## [1] 1134358
sum(is.na(ny_noaa$tmin))
## [1] 1134420
```

``` r
ny_noaa_df = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = '-') %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)) %>%
  mutate(
    prcp = prcp / 10, 
    tmin = tmin / 10,
    tmax = tmax / 10) # Ensure observations for temperature, precipitation, and snowfall are given in reasonable units?
```

NOTE ABOUT THE SNOW VARIABLE FROM DISCUSSION BOARD

As for the final plot, you should filter values based on the original
definition of snowfall (i.e., using the original units). To that end,
you might want to consider retaining both the original variable snow as
well as your proposal for a variable with a more sensical unit. As long
as you’re clear about how you label values and units in your
visualizations, you should be good!

Hello, Haochen!

You should make a single plot for part (i) and a single plot for part
(ii). Then, you should combine these two plots side by side into a
single-panel exhibit; you can find a refresher on how to do that
hereLinks to an external site.. For the first plot, you are correct that
“vs” implies tmax should be on the vertical axis and that tmin should be
on the horizontal axis.

## Dataset questions:

### Most common values of snowfall

#### Distribution histogram

``` r
ggplot(ny_noaa_df, aes(x = snow)) + geom_histogram()
## Warning: Removed 381221 rows containing non-finite values (stat_bin).
```

![](p8105_hw3_sef2183_files/figure-gfm/problem%203%20snowfall%20plot-1.png)<!-- -->

#### Table

``` r
ny_noaa_df %>%
  count(snow, name = "count") %>% 
  knitr::kable()
```

|  snow |   count |
|------:|--------:|
|   -13 |       1 |
|     0 | 2008508 |
|     3 |    8790 |
|     5 |    9748 |
|     8 |    9962 |
|    10 |    5106 |
|    13 |   23095 |
|    15 |    3672 |
|    18 |    3226 |
|    20 |    4797 |
|    23 |    1959 |
|    25 |   31022 |
|    28 |    2118 |
|    30 |    2814 |
|    33 |    2380 |
|    36 |    1630 |
|    38 |    9197 |
|    41 |    1467 |
|    43 |    1337 |
|    46 |    2123 |
|    48 |     918 |
|    51 |   18274 |
|    53 |    1155 |
|    56 |    1179 |
|    58 |    1198 |
|    61 |     849 |
|    64 |    4506 |
|    66 |     790 |
|    69 |     726 |
|    71 |    1075 |
|    74 |     463 |
|    76 |   10173 |
|    79 |     635 |
|    81 |     811 |
|    84 |     553 |
|    86 |     476 |
|    89 |    2535 |
|    91 |     428 |
|    94 |     404 |
|    97 |     704 |
|    99 |     276 |
|   102 |    6552 |
|   104 |     349 |
|   107 |     504 |
|   109 |     393 |
|   112 |     243 |
|   114 |    1578 |
|   117 |     276 |
|   119 |     248 |
|   122 |     411 |
|   124 |     183 |
|   127 |    3901 |
|   130 |     217 |
|   132 |     310 |
|   135 |     253 |
|   137 |     173 |
|   140 |     994 |
|   142 |     187 |
|   145 |     172 |
|   147 |     268 |
|   150 |     124 |
|   152 |    3131 |
|   155 |     186 |
|   157 |     209 |
|   160 |     149 |
|   163 |     133 |
|   165 |     614 |
|   168 |     115 |
|   170 |     104 |
|   173 |     187 |
|   175 |      80 |
|   178 |    1650 |
|   180 |      93 |
|   183 |     132 |
|   185 |     117 |
|   188 |      77 |
|   191 |     426 |
|   193 |      70 |
|   196 |      75 |
|   198 |     130 |
|   201 |      60 |
|   203 |    1475 |
|   206 |      74 |
|   208 |      98 |
|   211 |      69 |
|   213 |      58 |
|   216 |     292 |
|   218 |      55 |
|   221 |      53 |
|   224 |      61 |
|   226 |      35 |
|   229 |     744 |
|   231 |      43 |
|   234 |      52 |
|   236 |      49 |
|   239 |      39 |
|   241 |     192 |
|   244 |      36 |
|   246 |      37 |
|   249 |      58 |
|   251 |      21 |
|   254 |     786 |
|   257 |      34 |
|   259 |      48 |
|   262 |      28 |
|   264 |      24 |
|   267 |     130 |
|   269 |      19 |
|   272 |      22 |
|   274 |      45 |
|   277 |      20 |
|   279 |     369 |
|   282 |      28 |
|   284 |      37 |
|   287 |      22 |
|   290 |      24 |
|   292 |      81 |
|   295 |      20 |
|   297 |      14 |
|   300 |      24 |
|   302 |      22 |
|   305 |     451 |
|   307 |      17 |
|   310 |      29 |
|   312 |      22 |
|   315 |      13 |
|   318 |      70 |
|   320 |       7 |
|   323 |      22 |
|   325 |      12 |
|   328 |       6 |
|   330 |     226 |
|   333 |       9 |
|   335 |      13 |
|   338 |      17 |
|   340 |      13 |
|   343 |      63 |
|   345 |      17 |
|   348 |       6 |
|   351 |      15 |
|   353 |      12 |
|   356 |     235 |
|   358 |      12 |
|   361 |      15 |
|   363 |      14 |
|   366 |      15 |
|   368 |      32 |
|   371 |       4 |
|   373 |       6 |
|   376 |      12 |
|   378 |       5 |
|   381 |     139 |
|   384 |       6 |
|   386 |       8 |
|   389 |       5 |
|   391 |       1 |
|   394 |      27 |
|   396 |       5 |
|   399 |       4 |
|   401 |      10 |
|   404 |       7 |
|   406 |     116 |
|   409 |       6 |
|   411 |       8 |
|   414 |      12 |
|   417 |       9 |
|   419 |      15 |
|   422 |       5 |
|   424 |       3 |
|   427 |       8 |
|   429 |       1 |
|   432 |      63 |
|   434 |       7 |
|   437 |       8 |
|   439 |       3 |
|   445 |       8 |
|   447 |       5 |
|   450 |       5 |
|   452 |       5 |
|   455 |       4 |
|   457 |     100 |
|   460 |       5 |
|   462 |       3 |
|   465 |       5 |
|   467 |       6 |
|   470 |      20 |
|   472 |       4 |
|   475 |       5 |
|   478 |       4 |
|   480 |       2 |
|   483 |      44 |
|   488 |       4 |
|   490 |       2 |
|   495 |       3 |
|   498 |       2 |
|   503 |       2 |
|   505 |       2 |
|   508 |      54 |
|   511 |       2 |
|   513 |       3 |
|   516 |       2 |
|   518 |       3 |
|   521 |       8 |
|   523 |       2 |
|   526 |       2 |
|   528 |       2 |
|   533 |      16 |
|   536 |       1 |
|   544 |       1 |
|   546 |       6 |
|   549 |       4 |
|   551 |       2 |
|   554 |       4 |
|   556 |       1 |
|   559 |      35 |
|   561 |       2 |
|   564 |       2 |
|   566 |       1 |
|   569 |       1 |
|   572 |       3 |
|   574 |       1 |
|   577 |       1 |
|   579 |       1 |
|   584 |      20 |
|   587 |       1 |
|   589 |       1 |
|   592 |       2 |
|   594 |       3 |
|   597 |       4 |
|   607 |       1 |
|   610 |      35 |
|   612 |       1 |
|   615 |       1 |
|   620 |       1 |
|   622 |       2 |
|   625 |       1 |
|   630 |       2 |
|   632 |       3 |
|   635 |      10 |
|   643 |       2 |
|   645 |       1 |
|   648 |       1 |
|   650 |       1 |
|   660 |      13 |
|   663 |       2 |
|   665 |       1 |
|   686 |       6 |
|   693 |       1 |
|   699 |       4 |
|   704 |       1 |
|   711 |      10 |
|   721 |       2 |
|   734 |       1 |
|   737 |       9 |
|   754 |       1 |
|   762 |      17 |
|   775 |       3 |
|   787 |       4 |
|   808 |       1 |
|   810 |       1 |
|   813 |       2 |
|   838 |       2 |
|   843 |       1 |
|   861 |       1 |
|   864 |       2 |
|   871 |       1 |
|   892 |       1 |
|   914 |       4 |
|   940 |       1 |
|   953 |       1 |
|   965 |       1 |
|   978 |       1 |
|  1041 |       1 |
|  1067 |       1 |
|  1105 |       1 |
|  1143 |       1 |
|  1207 |       1 |
|  6350 |       1 |
|  7122 |       1 |
|  7765 |       1 |
| 10160 |       1 |
|    NA |  381221 |

From both the distribution histogram of snowfall and the table with the
counts for each value of snowfall, the most commonly observed value of
snowfall is \_\_\_\_\_\_\_\_\_. The reason is because
\_\_\_\_\_\_\_\_\_\_\_.

## Plot of average max temperature in January and in July in each station across years

Make a two-panel plot

You should compute the average for max temperature for each station
separately and then plot the two plots side by side.

For this problem, you want to find the average of tmax for every unique
combination of station ID, year, and month (perhaps consider grouping by
these three variables to obtain the desired result). Then, you will want
to construct two plots - one for January and one for July - that show
the average maximum temperature (vertical axis) against years
(horizontal axis) for every station.

As you have all intuited, a scatterplot is way too busy to visualize all
of these data. To give you an idea of a better option, consider
something like a spaghetti plotLinks to an external site.. As Cathy also
points out, it is going to be best to suppress the legend since there
are so many possible labels for station ID. That’s okay, though, as we
just want to see if there are any meaningful trends that can be teased
out via the spaghetti plot.

``` r
jan_tmax_p = ny_noaa_df %>%
  filter(month == "01") %>% 
  group_by(year) %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Average max temperature in January in each station across years")

jul_tmax_p = ny_noaa_df %>%
  filter(month == "07") %>% 
  group_by(year) %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Average max temperature in July in each station across years")

jan_tmax_p + jul_tmax_p
```

![](p8105_hw3_sef2183_files/figure-gfm/problem%203%20max%20temp%20plots-1.png)<!-- -->

ny_noaa_tidy %\>% filter( month %in% c(“January”,“July”)) %\>%
group_by(month, year, id) %\>% summarize( tmax_mean = mean(tmax, na.rm =
TRUE) ) %\>% ggplot(aes(x = year, y = tmax_mean, color = id)) +
geom_bar(alpha = 0.5) + facet_grid(.\~month)

2)  In your ggplot aesthetic call, you might consider replacing “color =
    id” with “group = id”, as R could be struggling to come up with many
    colors if there are a lot of unique stations in the dataset.

<!-- -->

3)  Relatedly, it might be better to use a line plot instead of a bar
    plot, as there could be too many bars to display things properly.
    Since our horizontal axis is time, a line plot is reasonable.

-   There \[IS/IS NOT\] an observable interpretable structure
-   There \[ARE/ARE NOT\] outliers

## Plot of tmax vs tmin for the full dataset (panel 1) and the distribution of snowfall values greater than 0 and less than 100 separately by year (panel 2)

Make a two-panel plot showing

We want to limit the values of snowfall to the criteria specified and
then use ggplot to plot the distribution (density plot).

``` r
i_p = ny_noaa_df %>%
  filter(month == "01") %>% 
  ggplot(aes(x = tmin, y = tmax)) +  
  theme(legend.position = "bottom") + geom_hex() +
  ggtitle("Max temp vs min temperature")

ii_p = ny_noaa_df %>%
  filter(snow < 0 | snow > 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Distribution of extreme snowfall values by year")

i_p + ii_p
## Warning: Removed 93189 rows containing non-finite values (stat_binhex).
```

![](p8105_hw3_sef2183_files/figure-gfm/problem%203%20tmax%20vs%20tmin%20and%20snowfall%20plots-1.png)<!-- -->
