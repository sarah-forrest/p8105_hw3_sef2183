---
title: "p8105_hw3_sef2183"
author: "Sarah Forrest"
date: "2022-10-06"
output: github_document
---

# Problem 0

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

```{r load packages}
library(tidyverse)
library(patchwork)
```

# Problem 1

```{r read in problem 1 data}
library(p8105.datasets)
data("instacart")
```

## Description of the dataset:
* Size and structure of the data
* Describe key variables
* Illustrative examples of observations.

## Dataset questions (commenting on the results of each):
* How many aisles are there, and which aisles are the most items ordered from?
* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. 
* Arrange aisles sensibly, and organize your plot so others can read it.
* Make a table showing the three most popular items in *each of the aisles* “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

# Problem 2
Below, the accelerometer data from `accel_data.csv` is read in and the variable names are cleaned using `clean_names()`. Then, `pivot_longer()` is used to manipulate the dataset from wide to long format. To do this, all variable names beginning with "activity" (`activity_1` to `activity_1440`) were transformed into 2 different variables: one denoting the minute of the day and one denoting the activity measure.The variable names were changed to `minute` and the prefix "activity_" was removed so that the variable values contained only the minute number, and the values were changed to `activity_count`, indicating the amount of activity read by the accelerometer during each 1 minute window. Finally, a new variable called `weekday_weekend` was created to indicate whether the day of the week is a weekday or a weekend day. 

```{r read and manipulate problem 2 data}
accel_df = 
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    starts_with("activity"),
    names_to = "minute",
    names_prefix = "activity_", 
    values_to = "activity_count") %>%
  mutate(weekday_weekend = case_when(day == "Monday" ~ "weekday",
                                     day == "Tuesday" ~ "weekday",
                                     day == "Wednesday" ~ "weekday",
                                     day == "Thursday" ~ "weekday",
                                     day == "Friday" ~ "weekday",
                                     day == "Saturday" ~ "weekend",
                                     day == "Sunday" ~ "weekend"))

# What is meant by encode data with reasonable variable classes
```

## Dataset description
* The variables in the resulting dataset include: `week`, `day_id` (day count number), `day` (day of the week), `minute`, `activity_count`, and `weekday_weekend` (either weekday or weekend)
* The resulting dataset contains `r nrow(accel_df)` rows/observations and `r ncol(accel_df)` columns/variables.

## Table of total activity over each day
Below, the tidied accelerometer dataset was aggregated across minutes using `group_by()` to group the data together by the `day_id` variable. Then a `total_activity` variable was created by summing the `activity_count` variables for each 1 minute window throughout the day to indicate the total activity read by the accelerometer during each day of the study. Finally, only the relevant variables `day_id`, `week`, `day`, and `total_activity` were selected from the dataset using `select()` and a table was created to show the daily totals for each of the 35 days of the study using `kable()` from the `knitr` package. 

```{r problem 2 total activity table}
accel_df %>%
  group_by(day_id, week, day) %>%
  summarize(
    total_activity = sum(activity_count)) %>% 
  select(day_id, week, day, total_activity) %>% 
  knitr::kable()
```

* It's very difficult to see any trends that are apparent from this table. I immediately noticed that the last 2 Saturdays in the study (days 24 and 31) had a much lower total activity count than the other days in the study. However, the previous Saturdays in the study had relatively similar total activity counts to the rest of the days in the study, so this isn't a trend that can be seen through the entire dataset. Visualization may be required to see any trends, if they are apparent in this dataset. 

## Plot of the 24-hour activity time courses for each day 
Below, a single panel-plot was created using `ggplot()` to show the 24-hour activity time courses for each day, using color to indicate the day of the week. `group_by()` was used to group the data by day of the week, then `ggplot()` was called to create a scatterplot with `minute` on the x axis and `activity_count` (data read from the accelerometer in 1 minute intervals) on the y axis.

```{r problem 2 plot}
accel_df %>%
  group_by(day) %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) + geom_point() + 
  theme(legend.position = "bottom")
```

This plot shows the individual's activity course throughout the entire 1440 minutes within a 24-hour period for each day of the week throughout the 5 weeks if the study observation period. However, since there are so many minutes in a 24-hour period, the plot is messy and the values on the x axis are difficult to read. Even with this difficulty, it is still possible to see some patterns and trends in the data:

* For all days of the week, there are lower activity counts that can be observed mid morning, at around 9:30am-10:30am (estimated, because exact minutes can not be read on the x axis)
* On Tuesdays, the individual doesn't appear to spend any minutes with activity counts over 3750, indicating that the individual seems to have less/lighter activity on Tuesdays.
* On Sundays, the individual has most of their high activity counts later in the day compared to the other days of the week at around 5:00pm (estimated)
* On Thursdays, the individual has most of their high activity counts around midday at around 1:00pm (estimated)
* On Fridays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated), with some high activity in the early afternoon as well at around 3:00pm (estimated)
* On Mondays and Wednesdays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated).
* On Saturdays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated) and a some at late at night as well, at around 12:00am (estimated)

# Problem 3

```{r read in problem 3 data}
library(p8105.datasets)
data("ny_noaa")
```

## Description of the dataset:
* The size of the dataset is 2,595,176 observations (rows) and variables 7 variables (columns)
* The structure of the dataset is in long format, with 1 row for each observation date for each weather station.
* Variables in the dataset include: `id` (weather station ID), `date` (observation date), `prcp` (precipitation measured in tenths of a mm), `snow` (snowfall measured in mm), `snwd` (snow depth measured in mm), `tmax` (maximum temperature measured in tenths of degrees C), and `tmin` (minimum temperature measured in tenths of degrees C). 
* The dataset contains extensive missing data, since each weather station may collect only a subset of the weather variables:

```{r problem 3 missing data}
sum(is.na(ny_noaa$prcp)) 
sum(is.na(ny_noaa$snow))
sum(is.na(ny_noaa$snwd))
sum(is.na(ny_noaa$tmax))
sum(is.na(ny_noaa$tmin))

# MAYBE USE PULL INSTEAD HERE?
```

* There are 145,838 missing observations for `prcp`
* There are 381,221 missing observations for `snow`
* There are 591,786 missing observations for `snwd`
* There are 1,134,358 missing observations for `tmax`
* There are 1,134,420 missing observations for `tmin`

It's worth noting that the `tmax` and `tmin` variables have the most missing observations, and `prcp` has the least number of missing observations. This alligns with the dataset description which stated that about one half of the stations report precipitation only. Therefore, missing data will be more of a problem for analyses involving `tmax` and `tmin`, but less of a problem for analyses with the other variables in the dataset.

Below, a dataframe is created from the `ny_noaa` dataset  and the variable names are cleaned using the `clean_names()` function from the `janitor` package. Then, separate variables for `year`, `month`, and `day` were created by separating the `date` variable using `separate()` and indicating "-" as the delimiter. Then, `mutate()` was used on `tmax` and `tmin` first to convert them to numeric variables using `as.numeric()`, and then to convert `tmax`, `tmin`, and `prcp` to reasonable units. In the description of the original `ny_noaa` dataset, `tmax`, `tmin`, and `prcp` were reported in tenths of mm and tenths of degrees Celsius, which is not very easy to interpret. Therefore, the values were divided by 10 so the variables were in  reasonable units. 

```{r manipulate problem 3 data}
ny_noaa_df = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = '-') %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)) %>%
  mutate(
    prcp = prcp / 10, 
    tmin = tmin / 10,
    tmax = tmax / 10) 
```

MAYBE NEED TO REMOVE VALUES OF LESS THAN 0 AT THE END OF THIS STEP

## Dataset questions:
### Most common values of snowfall
#### Distribution histogram

Below is a distribution histogram of values for the snowfall variable `snow` using `ggplot()`, with values for `snow` on the x axis and a count on the y axis:
```{r problem 3 snowfall plot}
ggplot(ny_noaa_df, aes(x = snow)) + geom_histogram()
```

#### Table

Below is a table denoting the counts for each value of the snowfall variable `snow` using `count()` and the `kable()` function from the `knitr` package to create the table: 
```{r problem 3 snowfall count}
ny_noaa_df %>%
  count(snow, name = "count") %>% 
  knitr::kable()
```

From both the distribution histogram of snowfall and the table with the counts for each value of snowfall, the most commonly observed value of snowfall is 0, with a count of 2,008,508. The second most common value is NA with a count of 381,221. The reason that NA is a commonly observed value for `snow` is because about one half of the stations report precipitation only, and do not report data on the other weather variables including `snow`. This results in many missing or "NA" observations for `snow`. Additionally, the reason that 0 is the most commonly observed value for `snow` is because the dataset contains data from New York state weather stations only. While it does snow sometimes during the winter in New York state, it is not snowing for the majority of the year. This results in many values of 0 for the amount of snowfall for dates during the spring, summer, and fall months. 

****NOTE: Running this analysis revealed that there is 1 record with a value of -13 for `snow`. This indicates that the dataset may require additional cleaning, as this value is outside of the reasonable range because the amount of snowfall cannot be less than 0. CHECK ON THIS ABOVE

## Plot of average max temperature in January and in July in each station across years

Below, a two-panel scatterplot depicting the average maximum temperature in the months of January and in July in each station across years are side by side. To create the plot, separate objects were created for the January and July plots using the `filter()` function for month values of "01" or "07". Additionally, `id`, `year`, and `month` were grouped together using `group_by()` in order to calculate the mean of `tmax` for every unique combination of those 3 variables. A new `mean_tmax` variable was created using `mean()` within the `mutate()` function in combination with the grouping stated previously to calculate the mean maximum temperature for each station separately across each year. Then, `ggplot()` was used to plot points with `year` on the x axis and `mean_max` on the y axis. `theme()` was used to rotate the text on the x axis to make it easier to read and the legend was surpressed because the 700 stations made it too large. Finally, a short title was added to each plot using `ggtitle()` and then the two plots were arranged side by side.

```{r problem 3 max temp geom point plots}
jan_tmax_point = 
  ny_noaa_df %>%
  group_by(id, year, month) %>% 
  filter(month == "01") %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + geom_point(show.legend = FALSE) +
  ggtitle("Avg max temp in January")

jul_tmax_point = 
  ny_noaa_df %>%
  group_by(id, year, month) %>% 
  filter(month == "07") %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point(show.legend = FALSE) +
  ggtitle("Avg max temp in July")

jan_tmax_point + jul_tmax_point
```

Additionally, the same data is plotted below except using `geom_line` in order to visualize the trends a bit easier:

```{r problem 3 max temp geom line plots}
jan_tmax_line = 
  ny_noaa_df %>%
  group_by(id, year, month) %>% 
  filter(month == "01") %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + geom_line(show.legend = FALSE) + ylab("mean maximum temperature (C)") + ggtitle("Avg max temp in January")

jul_tmax_line = 
  ny_noaa_df %>%
  group_by(id, year, month) %>% 
  filter(month == "07") %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_line(show.legend = FALSE) + ylab("mean maximum temperature (C)") + ggtitle("Avg max temp in July")

jan_tmax_line + jul_tmax_line
```

* There is somewhat of an observable structure in the data. The mean maximum temperature for both the months of January and July oscillate higher and lower, with high peaks occurring roughly every 4 years. Most of the data follows the same general oscillating pattern, however there are some outliers.
* There are outliers in the data for both months. In January, there was a weather station that recorded a very low mean average temperature of about -16 degrees C in 1982. In July, there was also a weather station that recorded a low mean average temperature of about 19 degrees C in 1984, an even temperature of about 14 degrees C around 1988, and outliers of about 18 and 19 degrees C in 2004 and 2007.

## Plot of tmax vs tmin for the full dataset (panel 1) and the distribution of snowfall values greater than 0 and less than 100 separately by year (panel 2)

Below, a two-panel plot depicting the minimum temperature vs. the maximum temperature for the full dataset as well as the distribution of snowfall values greater than 0 and less than 100 are displayed side by side. For the first plot, an object was created and `ggplot()` was used to plot the maximum temperatures against the minimum temperatures in the entire dataset. Rather than a scatterplot, a hexagonal heatmap was created using `geom_hex()` to display the density coloring of the data points. For the second plot, the dataset was filtered to include only extreme values of snowfall less than 0 and greater than 100 using `filter()`, and grouped by year using `group_by()`. `ggplot()` was used to create a  distribution (density) plot using `geom_violin()`. Finally, the two panels were plotted side by side.

```{r problem 3 tmax vs tmin and snowfall plots}
i_p = ny_noaa_df %>%
  ggplot(aes(x = tmin, y = tmax)) +  
  theme(legend.position = "bottom") + geom_hex() + xlab("minimum temperature (C)") + ylab("maximum temperature (C)") + 
  ggtitle("Max temp vs. min temp")

ii_p = ny_noaa_df %>%
  filter(snow < 0 | snow > 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_violin() + ylab("snowfall (mm)") +
  ggtitle("Extreme snowfall by year")

i_p + ii_p
```
