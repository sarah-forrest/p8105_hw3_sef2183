---
title: "p8105_hw3_sef2183"
author: "Sarah Forrest"
date: "2022-10-06"
output: github_document
---

# Problem 0

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

```{r load packages}
library(tidyverse)
library(patchwork)
```

# Problem 1

```{r read in problem 1 data}
library(p8105.datasets)
data("instacart")
```

## Description of the dataset:
* Size and structure of the data
* Describe key variables
* Illustrative examples of observations.

## Dataset questions (commenting on the results of each):
* How many aisles are there, and which aisles are the most items ordered from?
* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. 
* Arrange aisles sensibly, and organize your plot so others can read it.
* Make a table showing the three most popular items in *each of the aisles* “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

# Problem 2
Below, the accelerometer data from `accel_data.csv` is read in and the variable names are cleaned using `clean_names()`. Then, `pivot_longer()` is used to manipulate the dataset from wide to long format. To do this, all variable names beginning with "activity" (`activity_1` to `activity_1440`) were transformed into 2 different variables: one denoting the minute of the day and one denoting the activity measure.The variable names were changed to `minute` and the prefix "activity_" was removed so that the variable values contained only the minute number, and the values were changed to `activity_count`, indicating the amount of activity read by the accelerometer during each 1 minute window. Finally, a new variable called `weekday_weekend` was created to indicate whether the day of the week is a weekday or a weekend day. 

```{r read and manipulate problem 2 data}
accel_df = 
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    starts_with("activity"),
    names_to = "minute",
    names_prefix = "activity_", 
    values_to = "activity_count") %>%
  mutate(weekday_weekend = case_when(day == "Monday" ~ "weekday",
                                     day == "Tuesday" ~ "weekday",
                                     day == "Wednesday" ~ "weekday",
                                     day == "Thursday" ~ "weekday",
                                     day == "Friday" ~ "weekday",
                                     day == "Saturday" ~ "weekend",
                                     day == "Sunday" ~ "weekend"))

# What is meant by encode data with reasonable variable classes
```

## Dataset description
* The variables in the resulting dataset include: `week`, `day_id` (day count number), `day` (day of the week), `minute`, `activity_count`, and `weekday_weekend` (either weekday or weekend)
* The resulting dataset contains `r nrow(accel_df)` rows/observations and `r ncol(accel_df)` columns/variables.

## Table of total activity over each day
Below, the tidied accelerometer dataset was aggregated across minutes using `group_by()` to group the data together by the `day_id` variable. Then a `total_activity` variable was created by summing the `activity_count` variables for each 1 minute window throughout the day to indicate the total activity read by the accelerometer during each day of the study. Finally, only the relevant variables `day_id`, `week`, `day`, and `total_activity` were selected from the dataset using `select()` and a table was created to show the daily totals for each of the 35 days of the study using `kable()` from the `knitr` package. 

```{r problem 2 total activity table}
accel_df %>%
  group_by(day_id, week, day) %>%
  summarize(
    total_activity = sum(activity_count)) %>% 
  select(day_id, week, day, total_activity) %>% 
  knitr::kable()
```

* It's very difficult to see any trends that are apparent from this table. I immediately noticed that the last 2 Saturdays in the study (days 24 and 31) had a much lower total activity count than the other days in the study. However, the previous Saturdays in the study had relatively similar total activity counts to the rest of the days in the study, so this isn't a trend that can be seen through the entire dataset. Visualization may be required to see any trends, if they are apparent in this dataset. 

## Plot of the 24-hour activity time courses for each day 
Below, a single panel-plot was created using `ggplot()` to show the 24-hour activity time courses for each day, using color to indicate the day of the week. `group_by()` was used to group the data by day of the week, then `ggplot()` was called to create a scatterplot with `minute` on the x axis and `activity_count` (data read from the accelerometer in 1 minute intervals) on the y axis.

```{r problem 2 plot}
accel_df %>%
  group_by(day) %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) + geom_point() + 
  theme(legend.position = "bottom")
```

This plot shows the individual's activity course throughout the entire 1440 minutes within a 24-hour period for each day of the week throughout the 5 weeks if the study observation period. However, since there are so many minutes in a 24-hour period, the plot is messy and the values on the x axis are difficult to read. Even with this difficulty, it is still possible to see some patterns and trends in the data:

* For all days of the week, there are lower activity counts that can be observed mid morning, at around 9:30am-10:30am (estimated, because exact minutes can not be read on the x axis)
* On Tuesdays, the individual doesn't appear to spend any minutes with activity counts over 3750, indicating that the individual seems to have less/lighter activity on Tuesdays.
* On Sundays, the individual has most of their high activity counts later in the day compared to the other days of the week at around 5:00pm (estimated)
* On Thursdays, the individual has most of their high activity counts around midday at around 1:00pm (estimated)
* On Fridays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated), with some high activity in the early afternoon as well at around 3:00pm (estimated)
* On Mondays and Wednesdays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated).
* On Saturdays, the individual has most of their high activity counts earlier in the day at around 5:00am (estimated) and a some at late at night as well, at around 12:00am (estimated)

# Problem 3

```{r read in problem 3 data}
library(p8105.datasets)
data("ny_noaa")
```

## Description of the dataset:
* The size of the dataset is 2,595,176 observations (rows) and variables 7 variables (columns)
* The structure of the dataset is in long format, with 1 row for each observation date for each weather station.
* Variables in the dataset include: `id` (weather station ID), `date` (observation date), `prcp` (precipitation measured in tenths of a mm), `snow` (snowfall measured in mm), `snwd` (snow depth measured in mm), `tmax` (maximum temperature measured in tenths of degrees C), and `tmin` (minimum temperature measured in tenths of degrees C). 
* The dataset has quite a bit of missing data, which has the potential to create issues for future analysis:

```{r problem 3 missing data}
sum(is.na(ny_noaa$prcp)) 
sum(is.na(ny_noaa$snow))
sum(is.na(ny_noaa$snwd))
sum(is.na(ny_noaa$tmax))
sum(is.na(ny_noaa$tmin))
```

* There are 145,838 missing observations for `prcp`
* There are 381,221 missing observations for `snow`
* There are 591,786 missing observations for `snwd`
* There are 1,134,358 missing observations for `tmax`
* There are 1,134,420 missing observations for `tmin`

It's worth noting that the `tmax` and `tmin` variables have the most missing observations, and `prcp` has the least number of missing observations. Therefore, missing data will be more of a problem for analyses involving `tmax` and `tmin`, but less of a problem for analyses with the other variables in the dataset.

Below, a dataframe is created from the `ny_noaa` dataset  and the variable names are cleaned using the `clean_names()` function from the `janitor` package. Then, separate variables for `year`, `month`, and `day` were created by separating the `date` variable using `separate()` and indicating "-" as the delimiter. Then, `mutate()` was used on `tmax` and `tmin` first to convert them to numeric variables using `as.numeric()`, and then to convert `tmax`, `tmin`, and `prcp` to reasonable units. In the description of the original `ny_noaa` dataset, `tmax`, `tmin`, and `prcp` were reported in tenths of mm and tenths of degrees Celsius, which is not very easy to interpret. Therefore, the values were divided by 10 so the variables were in  reasonable units. 

```{r manipulate problem 3 data}
ny_noaa_df = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = '-') %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)) %>%
  mutate(
    prcp = prcp / 10, 
    tmin = tmin / 10,
    tmax = tmax / 10) 
```

## Dataset questions:
### Most common values of snowfall
#### Distribution histogram

```{r problem 3 snowfall plot}
ggplot(ny_noaa_df, aes(x = snow)) + geom_histogram()
```

#### Table
```{r problem 3 snowfall count}
ny_noaa_df %>%
  count(snow, name = "count") %>% 
  knitr::kable()
```

From both the distribution histogram of snowfall and the table with the counts for each value of snowfall, the most commonly observed value of snowfall is _________. The reason is because ___________.

## Plot of average max temperature in January and in July in each station across years
Make a two-panel plot

You should compute the average for max temperature for each station separately and then plot the two plots side by side. 

For this problem, you want to find the average of tmax for every unique combination of station ID, year, and month (perhaps consider grouping by these three variables to obtain the desired result). Then, you will want to construct two plots - one for January and one for July - that show the average maximum temperature (vertical axis) against years (horizontal axis) for every station.

As you have all intuited, a scatterplot is way too busy to visualize all of these data. To give you an idea of a better option, consider something like a spaghetti plotLinks to an external site.. As Cathy also points out, it is going to be best to suppress the legend since there are so many possible labels for station ID. That's okay, though, as we just want to see if there are any meaningful trends that can be teased out via the spaghetti plot.

```{r problem 3 max temp plots}
jan_tmax_p = ny_noaa_df %>%
  filter(month == "01") %>% 
  group_by(year) %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Average max temperature in January in each station across years")

jul_tmax_p = ny_noaa_df %>%
  filter(month == "07") %>% 
  group_by(year) %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Average max temperature in July in each station across years")

jan_tmax_p + jul_tmax_p
```

ny_noaa_tidy %>%
    filter(
    month %in% c("January","July")) %>% 
  group_by(month, year, id) %>% 
  summarize(
    tmax_mean = mean(tmax, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = year, y = tmax_mean, color = id)) +
  geom_bar(alpha = 0.5) +
  facet_grid(.~month)
  
2) In your ggplot aesthetic call, you might consider replacing "color = id" with "group = id", as R could be struggling to come up with many colors if there are a lot of unique stations in the dataset.
(3) Relatedly, it might be better to use a line plot instead of a bar plot, as there could be too many bars to display things properly. Since our horizontal axis is time, a line plot is reasonable.
* There [IS/IS NOT] an observable interpretable structure
* There [ARE/ARE NOT] outliers

## Plot of tmax vs tmin for the full dataset (panel 1) and the distribution of snowfall values greater than 0 and less than 100 separately by year (panel 2)
Make a two-panel plot showing 

We want to limit the values of snowfall to the criteria specified and then use ggplot to plot the distribution (density plot). 

```{r problem 3 tmax vs tmin and snowfall plots}
i_p = ny_noaa_df %>%
  filter(month == "01") %>% 
  ggplot(aes(x = tmin, y = tmax)) +  
  theme(legend.position = "bottom") + geom_hex() +
  ggtitle("Max temp vs min temperature")

ii_p = ny_noaa_df %>%
  filter(snow < 0 | snow > 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) + 
  theme(axis.text.x = element_text(angle = 90,hjust = 1)) + geom_point() +
  ggtitle("Distribution of extreme snowfall values by year")

i_p + ii_p
```

NOTE ABOUT THE SNOW VARIABLE FROM DISCUSSION BOARD

As for the final plot, you should filter values based on the original definition of snowfall (i.e., using the original units). To that end, you might want to consider retaining both the original variable snow as well as your proposal for a variable with a more sensical unit. As long as you're clear about how you label values and units in your visualizations, you should be good!

Hello, Haochen!

You should make a single plot for part (i) and a single plot for part (ii). Then, you should combine these two plots side by side into a single-panel exhibit; you can find a refresher on how to do that hereLinks to an external site.. For the first plot, you are correct that "vs" implies tmax should be on the vertical axis and that tmin should be on the horizontal axis.
